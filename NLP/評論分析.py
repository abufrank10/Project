# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1JsBZ-yhI-XvVGZHsMkt1Fd0SaxajLl
"""

!pip install --upgrade transformers

import torch
import transformers
import torch.nn.functional as F
from transformers import BertConfig
import torch.nn as nn
from torch.autograd import Variable
from sklearn.model_selection import train_test_split
import re
import time
import sys
import os
import pandas as pd
import numpy as np
import sklearn
from pandas.core.frame import DataFrame
from transformers import BertTokenizer, BertModel
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import torch.utils.data as Data
from transformers import BertForSequenceClassification
from transformers.utils.dummy_pt_objects import BertPreTrainedModel

df1 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_信義區.csv')
df2 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_101店.csv')
df3 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_南西店.csv')
df4 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_台中店.csv')
df5 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_天母店.csv')
df6 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_復興店.csv')
df7 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_新光A4店.csv')
df8 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_新生店.csv')
df9 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_新竹店.csv')
df10 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_板橋店.csv')
df11 = pd.read_csv('//content/drive/MyDrive/鼎泰豐/鼎泰豐_遠百信義A13店.csv')
df12 = pd.read_csv('/content/drive/MyDrive/鼎泰豐/鼎泰豐_高雄店.csv')
df = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12],ignore_index=True)
df.shape

## 全部的數據
five_df = df[df['scores']==5]
four_df = df[df['scores']==4]
three_df = df[df['scores']==3]
two_df = df[df['scores']==2]
one_df = df[df['scores']==1]
google = pd.concat([one_df,two_df,three_df,four_df,five_df],ignore_index=True)
google['scores'] = google['scores'].replace([5,4,3,2,1],[3,3,2,1,1]) # 5、4分改成3；3分改成2；2、1改成1
google['scores'].value_counts().plot(kind='bar')

## 平均後的數據
one_score_df = google[google['scores']==1].sample(n=681)
two_score_df = google[google['scores']==2].sample(n=681)
three_score_df = google[google['scores']==3].sample(n=681)
average_df=pd.concat([one_score_df,two_score_df,three_score_df])
average_df['scores'].value_counts().plot(kind='bar')

train_df, test_df=train_test_split(average_df, test_size=0.2)
train_df, val_df=train_test_split(train_df, test_size=0.2)
PRETRAINED_MODEL_NAME="hfl/chinese-bert-wwm"
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)

from transformers.models.transfo_xl.tokenization_transfo_xl import tokenize_numbers
class MyDataset(Dataset):
  def __init__(self, mode, dataX, tokenizer):
    self.mode=mode
    self.dataX=dataX
    self.tokenizer=tokenizer
    self.len=len(dataX)
  
  def __getitem__(self, index):
    if self.mode=='train':
      text=self.dataX['review'].iloc[index]
      label_id=self.dataX['scores'].iloc[index]
      label_tensor=torch.tensor(label_id)
    else:
      text=self.dataX['review'].iloc[index]
      label_id=self.dataX['scores'].iloc[index]
      label_tensor=torch.tensor(label_id)
    
    word_pieces=["[CLS]"]
    tokens=self.tokenizer.tokenize(text[:20])
    tokens=word_pieces+tokens
    ids=self.tokenizer.convert_tokens_to_ids(tokens)
    token_tensor=torch.tensor(ids)
    return (token_tensor, label_tensor)

  def __len__(self):
    return self.len

trainData=MyDataset('train', train_df, tokenizer)
valData=MyDataset('train', val_df, tokenizer)
testData=MyDataset('test', test_df, tokenizer)

def create_mini_batch(samples):
  tokens_tensors=[s[0] for s in samples]
  
  if samples[0][1] is not None:
    label_ids=torch.stack([s[1] for s in samples]) ## 將label的tensor串在一起
  else:
    label_ids=None
  
  tokens_tensors=pad_sequence(tokens_tensors, batch_first=True)
  masks_tensors=torch.zeros(tokens_tensors.shape,dtype=torch.long)
  masks_tensors=masks_tensors.masked_fill(tokens_tensors != 0, 1)
  return tokens_tensors, masks_tensors, label_ids

BATCH_SIZE = 128
trainDataLoader = DataLoader(trainData, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)

PRETRAINED_MODEL_NAME="hfl/chinese-bert-wwm"
NUM_LABELS=3
model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME,num_labels=NUM_LABELS,attention_probs_dropout_prob=0.2)

# class BertForSequenceClassification(BertPreTrainedModel):
#   def __init__(self, config, num_labels=3):
#     super(BertForSequenceClassification, self).__init__(config)
#     self.num_labels = config.num_labels
#     self.config = config

#     self.bert = BertModel(config)
#     self.dropout = nn.Dropout(config.hidden_dropout_prob)
#     self.classifier = nn.Linear(config.hidden_size, 7)
#     self.init_weights()

#   def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None,):
#     outputs = self.bert(
#             input_ids,
#             attention_mask=attention_mask,
#             token_type_ids=token_type_ids,
#             position_ids=position_ids,
#             head_mask=head_mask,
#             inputs_embeds=inputs_embeds,
#             output_attentions=output_attentions,
#             output_hidden_states=output_hidden_states,
#             return_dict=return_dict,
#         )

#     pooled_output = outputs[1]

#     pooled_output = self.dropout(pooled_output)
#     logits = self.classifier(pooled_output)

#     loss = None
#     if labels is not None:
#       loss_fct=nn.CrossEntropyLoss()
#       loss=loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
#     return (loss, logits, outputs.hidden_states, outputs.attentions,)

class CustomBertModel(nn.Module):
  def __init__(self):
    super(CustomBertModel, self).__init__()
    self.num_labels=3
    self.bert=BertModel.from_pretrained("hfl/chinese-bert-wwm")
    self.dropout=nn.Dropout(0.3)
    self.classifier1=nn.Linear(768,768)
    self.classifier2=nn.Linear(768,256)
    self.classifier3=nn.Linear(256,128)
    self.classifier4=nn.Linear(128,3)
  
  def forward(self, input_ids, attention_mask, labels):
    outputs=self.bert(input_ids, attention_mask=attention_mask)
    pooler_output=outputs[1]
    pooler_output = self.dropout(pooler_output)
    logits = self.classifier1(pooler_output)
    logits = self.dropout(logits)
    logits = F.relu(logits)
    logits = self.classifier2(logits)
    logits = self.classifier3(logits)
    logits = self.dropout(logits)
    logits = F.relu(logits)
    logits = self.classifier4(logits)
    if labels is not None:
      loss_fct = nn.CrossEntropyLoss()
      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    return loss, logits

models=CustomBertModel()

def get_predictions(model, dataloader, compute_acc=False):
  predictions = None
  correct = 0
  total = 0
    
  with torch.no_grad():
    for data in dataloader:
      if next(model.parameters()).is_cuda:
        tokens_tensors, masks_tensors, labels= [t.to("cuda:0") for t in data]
        labels = Variable(labels)-1

      outputs=model(input_ids=tokens_tensors, attention_mask=masks_tensors, labels=labels)
      logits=outputs[1]
      _, pred = torch.max(logits.data, 1)

      if compute_acc:
        total+=labels.size(0)
        correct+=(pred == labels).sum().item() ##对比后相同的值会为1，不同则会为，並加總起來轉成純量
      
      if predictions is None:
        predictions=pred
      else:
        predictions=torch.cat((predictions, pred))
    
  if compute_acc:
    acc=correct/total
    return predictions, acc
  return predictions

# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# print("device:", device)
# model=models.to(device)
# _, acc = get_predictions(model, trainDataLoader, compute_acc=True)
# print("classification acc:", acc)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("device:", device)
model=models.to(device)
model.train()

# 使用 Adam Optim 更新整個分類模型的參數
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

EPOCHS = 40  # 幸運數字
for epoch in range(EPOCHS):
  
  running_loss = 0.0
  for data in trainDataLoader:
    tokens_tensors, masks_tensors, labels= [t.to(device) for t in data]
    labels = Variable(labels)-1

    # 將參數梯度歸零
    optimizer.zero_grad()
    
    # forward pass
    outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors, labels=labels)

    loss = outputs[0]
    # backward
    loss.backward()
    optimizer.step()


    # 紀錄當前 batch loss
    running_loss += loss.item()
    
  # 計算分類準確率
  _, train_acc = get_predictions(model, trainDataLoader, compute_acc=True)
  _, val_acc = get_predictions(model, valDataLoader, compute_acc=True)

  print('[epoch %d] loss: %.3f, train_acc: %.3f, val_acc: %.3f' %
        (epoch + 1, running_loss, train_acc, val_acc))

_, test_acc = get_predictions(model, testDataLoader, compute_acc=True)
print(test_acc)